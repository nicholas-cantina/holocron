{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.handlebars' from '/Users/nicholasbern/src/holocron/utils/handlebars.py'>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Assume your modules are in the 'src' directory under the current notebook directory\n",
    "# Get the absolute path of the parent directory\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "# Add it to sys.path if not already included\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "from utils import common\n",
    "from utils import extract\n",
    "from utils import handlebars\n",
    "\n",
    "# necessary sometimes when importing in VSCode\n",
    "import importlib\n",
    "importlib.reload(common)\n",
    "importlib.reload(extract)\n",
    "importlib.reload(handlebars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../config.ini')\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=config['OPENAI']['OPENAI_API_KEY']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "memory_temperature = 0.25\n",
    "chat_temperature = 0.25\n",
    "\n",
    "memory_model = \"gpt-4o\"\n",
    "chat_model = \"gpt-4o\"\n",
    "\n",
    "def get_memories(prompt_data, questions, prompt_template):\n",
    "    memories = []\n",
    "\n",
    "    for question in questions[\"speaking_style\"]:\n",
    "        question_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"first_name\": \"Lore\",\n",
    "            \"full_name\": \"Lore Master\",\n",
    "            \"message\": question\n",
    "        }\n",
    "        prompt_data_for_memories = prompt_data.copy()\n",
    "        prompt_data_for_memories[\"messages\"] = [question_message]\n",
    "\n",
    "        memory_result = client.chat.completions.create(\n",
    "            messages=handlebars.make_prompt(prompt_template, prompt_data_for_memories),\n",
    "            model=memory_model,\n",
    "            temperature=memory_temperature\n",
    "        )\n",
    "\n",
    "        memories += question_message\n",
    "        memories += {\n",
    "            \"role\": \"assistant\",\n",
    "            \"first_name\": \"\", # not used\n",
    "            \"full_name\": \"\", # not used\n",
    "            \"message\": memory_result.choices[0].content\n",
    "        }\n",
    "    \n",
    "    return memories\n",
    "\n",
    "conversation_filepath = \"../data/multi-user-test-conversations.jsonl\"\n",
    "conversations = common.read_jsonl_file(conversation_filepath)\n",
    "\n",
    "questions_filepath = \"../data/questions.json\"\n",
    "questions = common.read_json_file(questions_filepath)\n",
    "\n",
    "prompt_template_filepath = \"../data/prompt-template.hbs\"\n",
    "prompt_template = common.read_file(prompt_template_filepath)\n",
    "\n",
    "how_many_conversations = 1\n",
    "\n",
    "results = []\n",
    "\n",
    "for conversation in itertools.islice(conversations, how_many_conversations):\n",
    "    prompt_data = extract.extract_prompt_data(conversation)\n",
    "    messages = prompt_data[\"messages\"]\n",
    "\n",
    "    memories = get_memories(prompt_data, questions, prompt_template)\n",
    "\n",
    "    no_memories_result = client.chat.completions.create(\n",
    "        messages=handlebars.make_prompt(prompt_template, prompt_data),\n",
    "        model=chat_model,\n",
    "        temperature=chat_temperature\n",
    "    ).choices[0].message[\"content\"]\n",
    "    with_memories_result = client.chat.completions.create(\n",
    "        messages=handlebars.make_prompt(prompt_template, {\n",
    "            **prompt_data,\n",
    "            \"memories\": memories + prompt_data[\"messages\"]\n",
    "        }),\n",
    "        model=chat_model,\n",
    "        temperature=chat_temperature\n",
    "    ).choices[0].message[\"content\"]\n",
    "\n",
    "    results.append({\n",
    "        \"conversation\": conversation,\n",
    "        \"no_memory\": no_memories_result,\n",
    "        \"with_memory\": with_memories_result\n",
    "    })\n",
    "\n",
    "common.save_json_file(\"../data/memories-test-results.json\", results)\n",
    "common.save_jsonl_file(\"../data/conversations.json\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
